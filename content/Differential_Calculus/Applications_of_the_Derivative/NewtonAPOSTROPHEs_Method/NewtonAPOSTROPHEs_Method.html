<div class="latex_section">
	Derivation
</div>
<div class="latex_body">
	By now it is known that a function can be approximated as:
	<div class="latex_equation">
		$\eqalign{
			f(x) = f(x_0) + f'(x_0)(x - x_0)
		}$
	</div>
	where $x_0$ is the point we choose to center ourselves at. Now what if we wanted to find the root of the function, $x = \alpha$, given some interval $[\alpha - r,\alpha + r]$ with $x = x_0$, where $x_0 \in [\alpha - r,\alpha + r]$, as the starting point? To accomplish this we set the function equal to zero and solve for $x$:
	<div class="latex_equation">
		$\eqalign{
			0 &= f(x_0) + f'(x_0)(x - x_0) \\
			0 &= f(x_0) + f'(x_0)x - f'(x_0)x_0 \\
			f'(x_0)x &= f'(x_0)x_0 - f(x_0) \\
			x &= x_0 - \frac{f(x_0)}{f'(x_0)}
		}$
	</div>
	where the result is exactly what we want, but when working with machines we typically like to use recursive algorithms so by discretization:
	<div class="latex_equation">
		$\eqalign{
			x_{n + 1} = x_n - \frac{f(x_n)}{f'(x_n)}
		}$
	</div>
	The result is known as $\textit{Newton's Method}$, which is one of the most widely known numerical algorithms used to determine the roots of a function. 
</div>

<div class="latex_section">
	Convergence
</div>
<div class="latex_body">
	Deriving the method itself and understanding it is certainly not that challenging, but discussing the convergence of the algorithm requires a bit more insight. This is typically accomplished by analyzing the behavior of the error sequence which we define as:
	<div class="latex_equation">
		$\eqalign{
			\epsilon_n = x - x_n
		}$
	</div>
	If the errors approach zero for larger values of $n$, then the algorithm converges to our final solution $x$. To begin I must first introduce a higher order approximation to the function. If we assume that our function can take the form:
	<div class="latex_equation">
		$\eqalign{
			f(x) = a_0 + a_1(x - x_0) + a_2(x - x_0)^2
		}$
	</div>
	then by differentiation:
	<div class="latex_equation">
		$\eqalign{
			f'(x) &= a_1 + 2a_2(x - x_0) \\
			f''(x) &= 2a_2
		}$
	</div>
	and evaluation at $x = x_0$ we determine that the coefficients have to be:
	<div class="latex_equation">
		$\eqalign{
			a_0 &= f(x_0) \\
			a_1 &= f'(x_0) \\
			a_2 &= \frac{f''(x_0)}{2}
		}$
	</div>
	which is nothing more than an extension of the linear approximation, specifically known as a $\textit{quadratic approximation}$. So we have:
	<div class="latex_equation">
		$\eqalign{
			f(x) = f(x_0) + f'(x_0)(x - x_0) + \frac{f''(x_0)}{2}(x - x_0)^2
		}$
	</div>
	where we assume that the second derivative is continuous. So at $x_n$:
	<div class="latex_equation">
		$\eqalign{
			f(x) = f(x_n) + f'(x_n)(x - x_n) + \frac{f''(x_n)}{2}(x - x_n)^2
		}$
	</div>
	Now equating the function to zero and applying the error defined earlier provides:
	<div class="latex_equation">
		$\eqalign{
			0 &= f(x) \\
			&= f(\epsilon_n + x_n) \\
			&= f(x_n) + f'(x_n)\epsilon_n + \frac{f''(x_n)}{2}\epsilon_n^2
		}$
	</div>
	and if $f'(x_n) \neq 0$ we may use the above to deduce:
	<div class="latex_equation">
		$\eqalign{
			0 &= f(x_n) + f'(x_n)\epsilon_n + \frac{f''(x_n)}{2}\epsilon_n^2 \\
			0 &= \frac{f(x_n)}{f'(x_n)} + \epsilon_n + \frac{f''(x_n)}{2f'(x_n)}\epsilon_n^2 \\
			-\frac{f(x_n)}{f'(x_n)} &= \epsilon_n + \frac{f''(x_n)}{2f'(x_n)}\epsilon_n^2
		}$
	</div>
	Finally we have everything we need, going back to the error definition and using the above we write:
	<div class="latex_equation">
		$\eqalign{
			\epsilon_{n + 1} &= x - x_{n + 1} \\
			&= x - \Big( x_n - \frac{f(x_n)}{f'(x_n)} \Big) \\
			&= x - \Big( x_n + \epsilon_n + \frac{f''(x_n)}{2f'(x_n)}\epsilon_n^2 \Big) \\
			&= -\frac{f''(x_n)}{2f'(x_n)}\epsilon_n^2 \\
			|\epsilon_{n + 1}| &= \frac{1}{2} \Bigg| \frac{f''(x_n)}{f'(x_n)} \Bigg| |\epsilon_n|^2
		}$
	</div>
	The last line is the one that shows the rate of convergence is quadratic due to the right side. Of course to say this we have to assume a couple of conditions:
	<div class="latex_equation">
		$\eqalign{
			&1. \hspace{.1cm} f'(x) \neq 0 \hspace{.3cm} \forall x \in [\alpha - r,\alpha + r] \\
			&2. \hspace{.1cm} f''(x) \hspace{.1cm} \text{is continuous} \hspace{.1cm} \forall x \in [\alpha - r,\alpha + r] \\
			&3. \hspace{.1cm} x_0, \hspace{.1cm} \text{the starting point}, \text{is sufficiently close to the root} \hspace{.1cm} \alpha
		}$
	</div>
	We can even go on to show explicitly that the sequence of errors is decreasing by determining an upper bound, $\Omega \in \mathbb{R}$, s.t.:
	<div class="latex_equation">
		$\eqalign{
			|\epsilon_{n + 1}| \leq \Omega |\epsilon_n|^2
		}$
	</div>
	To define the upper bound we can use our conclusion from above and choose the maximal value that the ratio of the two derivatives take:
	<div class="latex_equation">
		$\eqalign{
			\Omega = \sup_x \frac{1}{2} \Bigg| \frac{f''(x)}{f'(x)} \Bigg|
		}$
	</div>
</div>